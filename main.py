import streamlit as st
import pandas as pd
import numpy as np
import joblib
from urllib.parse import urlparse, parse_qs
from googleapiclient.discovery import build
from sklearn.feature_extraction.text import TfidfVectorizer
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import plotly.express as px
import matplotlib

# í•œê¸€ í°íŠ¸ ì„¤ì •
matplotlib.rcParams['font.family'] = 'Malgun Gothic'  # ìœˆë„ìš°
# matplotlib.rcParams['font.family'] = 'AppleGothic'  # Mac
matplotlib.rcParams['axes.unicode_minus'] = False

# YouTube API ì„¤ì •
API_KEY = "AIzaSyBKxUeZ41w7jBpj9GzKv0emNM7_4-V1e4Q"
youtube = build('youtube', 'v3', developerKey=API_KEY)

st.set_page_config(page_title="YouTube ëŒ“ê¸€ ê°ì • ë¶„ì„", layout="wide")
st.title("ğŸ¥ YouTube ëŒ“ê¸€ ê°ì • ë¶„ì„ê¸°")

# ìœ íŠœë¸Œ ë§í¬ ì…ë ¥
url = st.text_input("YouTube ì˜ìƒ ë§í¬ë¥¼ ì…ë ¥í•˜ì„¸ìš”")

# ì˜ìƒ ID ì¶”ì¶œ í•¨ìˆ˜
def extract_video_id(url):
    query = urlparse(url)
    if query.hostname == 'youtu.be':
        return query.path[1:]
    if query.hostname in ('www.youtube.com', 'youtube.com'):
        return parse_qs(query.query).get('v', [None])[0]
    return None

# ëŒ“ê¸€ ìˆ˜ì§‘ í•¨ìˆ˜
def get_comments(video_id):
    comments = []
    response = youtube.commentThreads().list(
        part="snippet",
        videoId=video_id,
        maxResults=100,
        textFormat="plainText"
    ).execute()
    for item in response['items']:
        comment = item['snippet']['topLevelComment']['snippet']['textDisplay']
        comments.append(comment)
    return comments

#ì¢‹ì•„ìš” ë§ì´ ë°›ì€ ëŒ“ê¸€ êµ¬í•˜ëŠ” í•¨ìˆ˜
def get_top_liked_comments(video_id, max_results=100):
    comments = []
    response = youtube.commentThreads().list(
        part="snippet",
        videoId=video_id,
        maxResults=max_results,
        textFormat="plainText"
    ).execute()

    for item in response["items"]:
        snippet = item["snippet"]["topLevelComment"]["snippet"]
        comments.append({
            "comment": snippet["textDisplay"],
            "likeCount": snippet.get("likeCount", 0)
        })

    # ì¢‹ì•„ìš” ê¸°ì¤€ ì •ë ¬
    df_comments = pd.DataFrame(comments)
    df_sorted = df_comments.sort_values(by="likeCount", ascending=False).head(10)
    return df_sorted

# ì˜ìƒ ì¢‹ì•„ìš” ê°œìˆ˜ êµ¬í•˜ëŠ” í•¨ìˆ˜
def get_video_likes(video_id):
    response = youtube.videos().list(
        part='statistics',
        id=video_id
    ).execute()
    stats = response['items'][0]['statistics']
    like = int(stats.get('likeCount', 0))
    return like

# ê°ì • ë¶„ì„ í•¨ìˆ˜ (ëª¨ë¸ ì‚¬ìš©)
def predict_sentiment(comments):
    vectorizer = joblib.load("model/vectorizer.pkl")
    model = joblib.load("model/sentiment_model.pkl")
    X = vectorizer.transform(comments)
    return model.predict(X)

# ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± í•¨ìˆ˜
def create_wordcloud(texts, title):
    text = ' '.join(texts)
    wordcloud = WordCloud(font_path='fonts/malgun.ttf', background_color='white', width=800, height=400).generate(text)
    fig, ax = plt.subplots()
    ax.imshow(wordcloud, interpolation='bilinear')
    ax.axis('off')
    st.subheader(title)
    st.pyplot(fig)

# ë„ë„› ì°¨íŠ¸ ìƒì„± í•¨ìˆ˜
def plot_donut(sentiments):
    df = pd.DataFrame(sentiments, columns=['label'])
    value_counts = df['label'].value_counts().reset_index()
    value_counts.columns = ['ê°ì •', 'ìˆ˜']
    fig = px.pie(value_counts, names='ê°ì •', values='ìˆ˜', hole=0.5, title='ê¸ì •/ë¶€ì • ê°ì • ë¹„ìœ¨')
    st.plotly_chart(fig)

# ë³¸ë¬¸ ì‹¤í–‰
if url:
    video_id = extract_video_id(url)
    if video_id:
        comments = get_comments(video_id)
        like = get_video_likes(video_id)

        # ì¢‹ì•„ìš” ì‹œê°í™”
        st.subheader("ğŸ‘ ì¢‹ì•„ìš” ìˆ˜")
        st.metric("ì¢‹ì•„ìš” ìˆ˜", f"{like:,}ê°œ")

        #ì¢‹ì•„ìš” ê°€ì¥ ë§ì´ ë°›ì€ ëŒ“ê¸€ 10ê°œ í‘œì‹œ
        st.subheader("ğŸ”¥ ì¢‹ì•„ìš” ë§ì€ ëŒ“ê¸€ TOP 10")
        top_comments = get_top_liked_comments(video_id)

        if not top_comments.empty:
            st.table(top_comments)
        else:
            st.info("ëŒ“ê¸€ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # ê°ì • ë¶„ì„
        sentiments = predict_sentiment(comments)

        # ê¸ì •/ë¶€ì • ì›Œë“œí´ë¼ìš°ë“œ
        positive_comments = [c for c, s in zip(comments, sentiments) if s == 'positive']
        #negative_comments = [c for c, s in zip(comments, sentiments) if s == 'negative']

        create_wordcloud(positive_comments, "ğŸ˜Š ê¸ì • ëŒ“ê¸€ ì›Œë“œí´ë¼ìš°ë“œ")
        create_wordcloud(negative_comments, "ğŸ˜  ë¶€ì • ëŒ“ê¸€ ì›Œë“œí´ë¼ìš°ë“œ")

        # ì „ì²´ ì›Œë“œí´ë¼ìš°ë“œ
        create_wordcloud(comments, "ğŸ’¬ ì „ì²´ ëŒ“ê¸€ ì›Œë“œí´ë¼ìš°ë“œ")

        # ê°ì • ë„ë„› ì°¨íŠ¸
        plot_donut(sentiments)
    else:
        st.error("ì˜¬ë°”ë¥¸ ìœ íŠœë¸Œ ë§í¬ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
